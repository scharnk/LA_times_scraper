{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NEWS_SCRAPYING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scharnk/LA_times_scraper/blob/main/NEWS_SCRAPYING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foeH02KoH0ER"
      },
      "source": [
        "# translate all news headlines to top languages of tourists\n",
        "# pull headlines from major journals across world... journals?\n",
        "\n",
        "# two translators: python3-translate OR\n",
        "\n",
        "# lxml 4.3.3 (min 3.4) CHECK\n",
        "# parsel 1.5.1\n",
        "# w3lib 1.20.0\n",
        "# twisted (min 14.0)\n",
        "# cryptography 2.6.1 CHECK\n",
        "# OpenSSL (min 0.14) CHECK\n",
        "# pyopenssl 19.0.0\n",
        "\n",
        "\n",
        "\n",
        "#import\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "#url = 'https://www.latimes.com/'\n",
        "#html = requests.get( url ).content\n",
        "#sel = Selector( text = html )\n",
        "\n",
        "#create spider class\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_title = response.xpath('//div/h5/a/text()').extract()\n",
        "        for title in lat_title:\n",
        "            print(title)\n",
        "        #return lat_title\n",
        "        #lat_headlines = lat_headlines.append(lat_title)\n",
        "\n",
        "# could include these sources as well:\n",
        "# response.xpath('//div/h4/a/text()').extract()\n",
        "# response.xpath('//div/h6/a/text()').extract()\n",
        "\n",
        "# lat_headlines = []\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(LAtimesSpider)\n",
        "process.start()\n",
        "\n",
        "\n",
        "\n",
        "# in case errors: \n",
        "# process.stop()\n",
        "# process.start()\n",
        "\n",
        "# print(lat_headlines)\n",
        "        #h5 class\n",
        "        #a href\n",
        "        #text\n",
        "\n",
        "        #<h1 class=\"spaced spaced-xl spaced-top spaced-bottom\">Tickets available Tuesday for Nipsey Hussle memorial service at Staples Center</h1>\n",
        "        #<h1 class=\"spaced spaced-xl spaced-top spaced-bottom\">Teams are searching Mt. Baldy area for two missing hikers</h1>\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiaX9b7xon3s"
      },
      "source": [
        "pip install scrapy;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1baq8IGHzxS"
      },
      "source": [
        "\n",
        "# scrape headlines from LA times with python scrapy\n",
        "\n",
        "# DEPENDENCIES:\n",
        "# lxml (min 3.4)\n",
        "# w3lib\n",
        "# twisted (min 14.0)\n",
        "# cryptography\n",
        "# OpenSSL (min 0.14)\n",
        "# pyopenssl\n",
        "\n",
        "\n",
        "#import\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "#create spider class\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_title = response.xpath('//div/h5/a/text()').extract()\n",
        "        for title in lat_title:\n",
        "            print(title)\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(LAtimesSpider)\n",
        "process.start()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXWGp9KFYNMh"
      },
      "source": [
        "# scrape headlines from LA times with python scrapy and send to touchdesigner\n",
        "\n",
        "# DEPENDENCIES:\n",
        "# lxml (min 3.4)\n",
        "# w3lib\n",
        "# twisted (min 14.0)\n",
        "# cryptography\n",
        "# OpenSSL (min 0.14)\n",
        "# pyopenssl\n",
        "\n",
        "# websockets\n",
        "# asyncio\n",
        "\n",
        "# import\n",
        "import asyncio\n",
        "import websockets\n",
        "import json\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# crawl latimes\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests(self):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        # for headline in lat_headlines:\n",
        "            # headlines.append(headline)\n",
        "\n",
        "        # LAtimesSpider.parse_headlines.headlinesOut = f\"{headlines}!\"\n",
        "\n",
        "# send headlines to touch\n",
        "async def wsComm(websocket, path):\n",
        "    while websockets.server.sockets:\n",
        "        get_headlines = await websocket.recv()\n",
        "        if get_headlines == 'true':\n",
        "            process = CrawlerProcess()\n",
        "            process.crawl(LAtimesSpider)\n",
        "            process.start()\n",
        "\n",
        "            # spyder = LAtimesSpider()\n",
        "            await websocket.send(LAtimesSpider.parse_headlines.headlinesOut)\n",
        "            print(f\"> {headlinesOut}\")\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "start_server = websockets.serve(wsComm, 'localhost', 8765)\n",
        "asyncio.get_event_loop().run_until_complete(start_server)\n",
        "asyncio.get_event_loop().run_forever()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHcIpokuYHIw"
      },
      "source": [
        "\n",
        "# scrape headlines from LA times with python scrapy\n",
        "\n",
        "# DEPENDENCIES:\n",
        "# lxml (min 3.4)\n",
        "# w3lib\n",
        "# twisted (min 14.0)\n",
        "# cryptography\n",
        "# OpenSSL (min 0.14)\n",
        "# pyopenssl\n",
        "\n",
        "# cached is too slow, on 4/16/19 3PM looked at latimes/entertainment/music page from 3/27/19 10:24PM\n",
        "\n",
        "# import time\n",
        "# import json\n",
        "\n",
        "from twisted.internet import reactor\n",
        "from twisted.internet import task \n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "#create spider class\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    download_delay = 5.0\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/entertainment/music/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        for headline in lat_headlines:\n",
        "            headlines.append(headline)\n",
        "        print(headlines)\n",
        "        # for headline in lat_headlines:\n",
        "        #     print(headline)\n",
        "\n",
        "timeout = 120\n",
        "\n",
        "def run_spider():\n",
        "    l.stop()\n",
        "    runner = CrawlerRunner(get_project_settings())\n",
        "    d = runner.crawl(LAtimesSpider)\n",
        "    d.addBoth(lambda _: l.start(timeout, False))\n",
        "\n",
        "l = task.LoopingCall(run_spider)\n",
        "l.start(timeout)\n",
        "\n",
        "reactor.run()\n",
        "\n",
        "\n",
        "\n",
        "# use next line instead of 'd = runner.crawl(LAtimesSpider)' if running more than one spider\n",
        "# d = runner.join()\n",
        "\n",
        "# put everything in a while True loop to run every 10 min (600 sec)\n",
        "# time.sleep(600)\n",
        "\n",
        "# can use schedule module to set when it runs\n",
        "\n",
        "\n",
        "\n",
        "# process = CrawlerProcess()\n",
        "# process.crawl(LAtimesSpider)\n",
        "# process.start()\n",
        "#\n",
        "# scrapy crawl latspider -o latspider.json\n",
        "\n",
        "\n",
        "import asyncio\n",
        "import websockets\n",
        "import json\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# crawl latimes\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests(self):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        # for headline in lat_headlines:\n",
        "            # headlines.append(headline)\n",
        "\n",
        "        # LAtimesSpider.parse_headlines.headlinesOut = f\"{headlines}!\"\n",
        "\n",
        "# allowed_domains = ['www.latimes.com/']\n",
        "# start_urls = ['https://www.latimes.com/', 'https://www.latimes.com/entertainment/music/']\n",
        "\n",
        "# lat_headlines = response.css('h5 > a::attr(href)::text').extract()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYKufylAYB0n"
      },
      "source": [
        "\n",
        "# scrape headlines from LA times with python scrapy\n",
        "\n",
        "# DEPENDENCIES:\n",
        "# lxml (min 3.4)\n",
        "# w3lib\n",
        "# twisted (min 14.0)\n",
        "# cryptography\n",
        "# OpenSSL (min 0.14)\n",
        "# pyopenssl\n",
        "\n",
        "# cached is too slow, on 4/16/19 3PM looked at latimes/entertainment/music page from 3/27/19 10:24PM\n",
        "\n",
        "# import time\n",
        "# import json\n",
        "\n",
        "from twisted.internet import reactor\n",
        "from twisted.internet import task\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "#create spider class\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    download_delay = 5.0\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "        # yield scrapy.Request(url = 'https://www.latimes.com/entertainment/music/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        for headline in lat_headlines:\n",
        "            headlines.append(headline)\n",
        "        print(headlines)\n",
        "\n",
        "class LAtimesmusicSpider(scrapy.Spider):\n",
        "    name = \"latmusicspider\"\n",
        "    download_delay = 5.0\n",
        "    def start_requests( self ):\n",
        "        # yield scrapy.Request(url = 'https://www.latimes.com/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/entertainment/music/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        music_headlines = []\n",
        "        lat_music_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        for music_headline in lat_music_headlines:\n",
        "            music_headlines.append(music_headline)\n",
        "        print(music_headlines)        \n",
        "\n",
        "timeout = 120\n",
        "\n",
        "def run_spider():\n",
        "    l.stop()\n",
        "    runner = CrawlerRunner(get_project_settings())\n",
        "    d = runner.crawl(LAtimesSpider)\n",
        "    d = runner.join()\n",
        "    d.addBoth(lambda _: l.start(timeout, False))\n",
        "\n",
        "l = task.LoopingCall(run_spider)\n",
        "l.start(timeout)\n",
        "\n",
        "reactor.run()\n",
        "\n",
        "\n",
        "\n",
        "# use next line instead of 'd = runner.crawl(LAtimesSpider)' if running more than one spider\n",
        "# d = runner.join()\n",
        "\n",
        "# put everything in a while True loop to run every 10 min (600 sec)\n",
        "# time.sleep(600)\n",
        "\n",
        "# can use schedule module to set when it runs\n",
        "\n",
        "\n",
        "\n",
        "# process = CrawlerProcess()\n",
        "# process.crawl(LAtimesSpider)\n",
        "# process.start()\n",
        "#\n",
        "# scrapy crawl latspider -o latspider.json\n",
        "\n",
        "from collections import Counter\n",
        "music = ['Anderson .Paak loves L.A. What does he do when it stops loving him back?', 'At Chella fest, Indio locals enjoy Coachella’s Latin lineup closer to home', 'Rapper Kodak Black arrested at U.S. border after guns and weed found, police say', \"Beyoncé has a surprise: 'Homecoming: The Live Album'\", 'Coachella showcased the power and joy of dance', 'Beyoncé’s ‘Homecoming’ Netflix doc captures an icon at her radical peak', 'The Game, Snoop Dogg push for Laura Ingraham’s firing after Nipsey Hussle diss', 'Nipsey Hussle memorial begins with beauty but has an ugly epilogue']\n",
        "musicstr = ''.join(music)\n",
        "split_it = musicstr.split()\n",
        "Counter = Counter(split_it)\n",
        "top_ten = Counter.most_common(10)\n",
        "print(top_ten)\n",
        "\n",
        "# [('at', 2), ('Hussle', 2), ('has', 2), ('after', 2), ('and', 2), ('an', 2), ('Chella', 1), ('of', 1), ('Anderson', 1), ('does', 1)]\n",
        "\n",
        "import nltk\n",
        "tokens = nltk.word_tokenize(musicstr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import asyncio\n",
        "import websockets\n",
        "import json\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# crawl latimes\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests(self):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        # for headline in lat_headlines:\n",
        "            # headlines.append(headline)\n",
        "\n",
        "        # LAtimesSpider.parse_headlines.headlinesOut = f\"{headlines}!\"\n",
        "\n",
        "# allowed_domains = ['www.latimes.com/']\n",
        "# start_urls = ['https://www.latimes.com/', 'https://www.latimes.com/entertainment/music/']\n",
        "\n",
        "# lat_headlines = response.css('h5 > a::attr(href)::text').extract()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dCVo7oT3-Qz"
      },
      "source": [
        "\n",
        "\n",
        "from twisted.internet import reactor\n",
        "from twisted.internet import task\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "class LAtimesmusicSpider(scrapy.Spider):\n",
        "    name = \"latmusicspider\"\n",
        "    download_delay = 5.0\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/entertainment/music/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        links = response.xpath('//div/div/h5/a/@href').extract()\n",
        "        for link in links:\n",
        "            yield response.follow( url = link, callback = self.parse_article )        \n",
        "        # music_headlines = []\n",
        "        # lat_music_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "    def parse_article(self, response):\n",
        "          # headline = response.xpath('//h1/text()').extract() #put here 4/9/20 KSS\n",
        "          # article = response.xpath('//p/text()').extract()\n",
        "          # image = response.xpath('//div/figure/div/img/@src').extract()\n",
        "        yield {\n",
        "          headline = response.xpath('//h1/text()').extract()\n",
        "          article = response.xpath('//p/text()').extract()\n",
        "          image = response.xpath('//div/figure/div/img/@src').extract()\n",
        "          }\n",
        "        for headline, image, article in zip(headline, image, article):\n",
        "            lat_dict[headline] = image, article       \n",
        "        for music_headline in lat_music_headlines:\n",
        "            music_headlines.append(music_headline)\n",
        "        print(music_headlines)\n",
        "\n",
        "lat_dict = dict()\n",
        "\n",
        "timeout = 120\n",
        "\n",
        "def run_spider():\n",
        "    l.stop()\n",
        "    runner = CrawlerRunner(get_project_settings())\n",
        "    d = runner.crawl(LAtimesSpider)\n",
        "    d = runner.join()\n",
        "    d.addBoth(lambda _: l.start(timeout, False))\n",
        "\n",
        "l = task.LoopingCall(run_spider)\n",
        "l.start(timeout)\n",
        "\n",
        "reactor.run()\n",
        "\n",
        "print(lat_dict)\n",
        "\n",
        "# def parse( self, response ):\n",
        "#     links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
        "#     # Follow each of the extracted links\n",
        "#     for link in links:\n",
        "#       yield response.follow( url = link, callback = self.parse_descr )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}