{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NEWS_SCRAPYING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scharnk/LA_times_scraper/blob/main/NEWS_SCRAPYING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foeH02KoH0ER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ad9135f7-79bc-469a-d058-77d542e7284c"
      },
      "source": [
        "# translate all news headlines to top languages of tourists\n",
        "# pull headlines from major journals across world... journals?\n",
        "\n",
        "# two translators: python3-translate OR\n",
        "\n",
        "# lxml 4.3.3 (min 3.4) CHECK\n",
        "# parsel 1.5.1\n",
        "# w3lib 1.20.0\n",
        "# twisted (min 14.0)\n",
        "# cryptography 2.6.1 CHECK\n",
        "# OpenSSL (min 0.14) CHECK\n",
        "# pyopenssl 19.0.0\n",
        "\n",
        "\n",
        "\n",
        "#import\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "#url = 'https://www.latimes.com/'\n",
        "#html = requests.get( url ).content\n",
        "#sel = Selector( text = html )\n",
        "\n",
        "#create spider class\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_title = response.xpath('//div/h5/a/text()').extract()\n",
        "        for title in lat_title:\n",
        "            print(title)\n",
        "        #return lat_title\n",
        "        #lat_headlines = lat_headlines.append(lat_title)\n",
        "\n",
        "# could include these sources as well:\n",
        "# response.xpath('//div/h4/a/text()').extract()\n",
        "# response.xpath('//div/h6/a/text()').extract()\n",
        "\n",
        "# lat_headlines = []\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(LAtimesSpider)\n",
        "process.start()\n",
        "\n",
        "\n",
        "\n",
        "# in case errors: \n",
        "# process.stop()\n",
        "# process.start()\n",
        "\n",
        "# print(lat_headlines)\n",
        "        #h5 class\n",
        "        #a href\n",
        "        #text\n",
        "\n",
        "        #<h1 class=\"spaced spaced-xl spaced-top spaced-bottom\">Tickets available Tuesday for Nipsey Hussle memorial service at Staples Center</h1>\n",
        "        #<h1 class=\"spaced spaced-xl spaced-top spaced-bottom\">Teams are searching Mt. Baldy area for two missing hikers</h1>\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-80a207ce9029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#url = 'https://www.latimes.com/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#html = requests.get( url ).content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiaX9b7xon3s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6981ff4-97d8-4ada-8572-43afa379fff5"
      },
      "source": [
        "pip install scrapy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/b7/06c19d7d7f5318ffd1d31d7cd7d944ed9dcf773981c731285350961d9b5c/Scrapy-2.0.1-py2.py3-none-any.whl (242kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.7MB/s \n",
            "\u001b[?25hCollecting queuelib>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Collecting cssselect>=0.9.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting Twisted>=17.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 7.6MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=16.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hCollecting zope.interface>=4.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 30.2MB/s \n",
            "\u001b[?25hCollecting service-identity>=16.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
            "Collecting cryptography>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/2d/29d2638b8df016526182594166c220913dafba3da0019b0776ff1bbc8ede/cryptography-2.9-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 27.3MB/s \n",
            "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting protego>=0.1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib>=1.17.0->scrapy) (1.12.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
            "Collecting Automat>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
            "Collecting incremental>=16.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting constantly>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (46.1.3)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
            "Building wheels for collected packages: PyDispatcher, protego\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11515 sha256=2b5409ea61b687a810bcf8d62848d6af81468f45f102ab14d2a8ab8324085d88\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protego: filename=Protego-0.1.16-cp36-none-any.whl size=7765 sha256=40f8e89c6986f2d7e89e67267188123d392edbf67ae20d451ecf266a243334bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/01/d1/4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n",
            "Successfully built PyDispatcher protego\n",
            "Installing collected packages: queuelib, w3lib, cssselect, Automat, zope.interface, incremental, constantly, hyperlink, PyHamcrest, Twisted, cryptography, pyOpenSSL, service-identity, parsel, PyDispatcher, protego, scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cryptography-2.9 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 parsel-1.5.2 protego-0.1.16 pyOpenSSL-19.1.0 queuelib-1.5.0 scrapy-2.0.1 service-identity-18.1.0 w3lib-1.21.0 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1baq8IGHzxS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd48f037-3bb5-4ba2-e756-b3ce6a555a50"
      },
      "source": [
        "\n",
        "# scrape headlines from LA times with python scrapy\n",
        "\n",
        "# DEPENDENCIES:\n",
        "# lxml (min 3.4)\n",
        "# w3lib\n",
        "# twisted (min 14.0)\n",
        "# cryptography\n",
        "# OpenSSL (min 0.14)\n",
        "# pyopenssl\n",
        "\n",
        "\n",
        "#import\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "#create spider class\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_title = response.xpath('//div/h5/a/text()').extract()\n",
        "        for title in lat_title:\n",
        "            print(title)\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(LAtimesSpider)\n",
        "process.start()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-10 05:49:33 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapybot)\n",
            "2020-04-10 05:49:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.6.9 (default, Nov  7 2019, 10:44:02) - [GCC 8.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.9, Platform Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-04-10 05:49:33 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-04-10 05:49:33 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2020-04-10 05:49:33 [scrapy.extensions.telnet] INFO: Telnet Password: 1714055f61b17714\n",
            "2020-04-10 05:49:33 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-04-10 05:49:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-04-10 05:49:33 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-04-10 05:49:33 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2020-04-10 05:49:33 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-04-10 05:49:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-04-10 05:49:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2020-04-10 05:49:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.latimes.com/> (referer: None)\n",
            "2020-04-10 05:49:33 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2020-04-10 05:49:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 215,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 67089,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 0.514389,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2020, 4, 10, 5, 49, 33, 937023),\n",
            " 'log_count/DEBUG': 1,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 182497280,\n",
            " 'memusage/startup': 182497280,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2020, 4, 10, 5, 49, 33, 422634)}\n",
            "2020-04-10 05:49:33 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVjdUl_bHm7h"
      },
      "source": [
        "https://scharnk.intelaedu.com/\n",
        "\n",
        "https://scharnk.intelaedu.com/wp-login.php?redirect_to=https%3A%2F%2Fscharnk.intelaedu.com%2Fwp-admin%2Findex.php&reauth=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXWGp9KFYNMh"
      },
      "source": [
        "# scrape headlines from LA times with python scrapy and send to touchdesigner\n",
        "\n",
        "# DEPENDENCIES:\n",
        "# lxml (min 3.4)\n",
        "# w3lib\n",
        "# twisted (min 14.0)\n",
        "# cryptography\n",
        "# OpenSSL (min 0.14)\n",
        "# pyopenssl\n",
        "\n",
        "# websockets\n",
        "# asyncio\n",
        "\n",
        "# import\n",
        "import asyncio\n",
        "import websockets\n",
        "import json\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# crawl latimes\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests(self):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        # for headline in lat_headlines:\n",
        "            # headlines.append(headline)\n",
        "\n",
        "        # LAtimesSpider.parse_headlines.headlinesOut = f\"{headlines}!\"\n",
        "\n",
        "# send headlines to touch\n",
        "async def wsComm(websocket, path):\n",
        "    while websockets.server.sockets:\n",
        "        get_headlines = await websocket.recv()\n",
        "        if get_headlines == 'true':\n",
        "            process = CrawlerProcess()\n",
        "            process.crawl(LAtimesSpider)\n",
        "            process.start()\n",
        "\n",
        "            # spyder = LAtimesSpider()\n",
        "            await websocket.send(LAtimesSpider.parse_headlines.headlinesOut)\n",
        "            print(f\"> {headlinesOut}\")\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "start_server = websockets.serve(wsComm, 'localhost', 8765)\n",
        "asyncio.get_event_loop().run_until_complete(start_server)\n",
        "asyncio.get_event_loop().run_forever()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHcIpokuYHIw"
      },
      "source": [
        "\n",
        "# scrape headlines from LA times with python scrapy\n",
        "\n",
        "# DEPENDENCIES:\n",
        "# lxml (min 3.4)\n",
        "# w3lib\n",
        "# twisted (min 14.0)\n",
        "# cryptography\n",
        "# OpenSSL (min 0.14)\n",
        "# pyopenssl\n",
        "\n",
        "# cached is too slow, on 4/16/19 3PM looked at latimes/entertainment/music page from 3/27/19 10:24PM\n",
        "\n",
        "# import time\n",
        "# import json\n",
        "\n",
        "from twisted.internet import reactor\n",
        "from twisted.internet import task \n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "#create spider class\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    download_delay = 5.0\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/entertainment/music/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        for headline in lat_headlines:\n",
        "            headlines.append(headline)\n",
        "        print(headlines)\n",
        "        # for headline in lat_headlines:\n",
        "        #     print(headline)\n",
        "\n",
        "timeout = 120\n",
        "\n",
        "def run_spider():\n",
        "    l.stop()\n",
        "    runner = CrawlerRunner(get_project_settings())\n",
        "    d = runner.crawl(LAtimesSpider)\n",
        "    d.addBoth(lambda _: l.start(timeout, False))\n",
        "\n",
        "l = task.LoopingCall(run_spider)\n",
        "l.start(timeout)\n",
        "\n",
        "reactor.run()\n",
        "\n",
        "\n",
        "\n",
        "# use next line instead of 'd = runner.crawl(LAtimesSpider)' if running more than one spider\n",
        "# d = runner.join()\n",
        "\n",
        "# put everything in a while True loop to run every 10 min (600 sec)\n",
        "# time.sleep(600)\n",
        "\n",
        "# can use schedule module to set when it runs\n",
        "\n",
        "\n",
        "\n",
        "# process = CrawlerProcess()\n",
        "# process.crawl(LAtimesSpider)\n",
        "# process.start()\n",
        "#\n",
        "# scrapy crawl latspider -o latspider.json\n",
        "\n",
        "\n",
        "import asyncio\n",
        "import websockets\n",
        "import json\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# crawl latimes\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests(self):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        # for headline in lat_headlines:\n",
        "            # headlines.append(headline)\n",
        "\n",
        "        # LAtimesSpider.parse_headlines.headlinesOut = f\"{headlines}!\"\n",
        "\n",
        "# allowed_domains = ['www.latimes.com/']\n",
        "# start_urls = ['https://www.latimes.com/', 'https://www.latimes.com/entertainment/music/']\n",
        "\n",
        "# lat_headlines = response.css('h5 > a::attr(href)::text').extract()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYKufylAYB0n"
      },
      "source": [
        "\n",
        "# scrape headlines from LA times with python scrapy\n",
        "\n",
        "# DEPENDENCIES:\n",
        "# lxml (min 3.4)\n",
        "# w3lib\n",
        "# twisted (min 14.0)\n",
        "# cryptography\n",
        "# OpenSSL (min 0.14)\n",
        "# pyopenssl\n",
        "\n",
        "# cached is too slow, on 4/16/19 3PM looked at latimes/entertainment/music page from 3/27/19 10:24PM\n",
        "\n",
        "# import time\n",
        "# import json\n",
        "\n",
        "from twisted.internet import reactor\n",
        "from twisted.internet import task\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "#create spider class\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    download_delay = 5.0\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "        # yield scrapy.Request(url = 'https://www.latimes.com/entertainment/music/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        for headline in lat_headlines:\n",
        "            headlines.append(headline)\n",
        "        print(headlines)\n",
        "\n",
        "class LAtimesmusicSpider(scrapy.Spider):\n",
        "    name = \"latmusicspider\"\n",
        "    download_delay = 5.0\n",
        "    def start_requests( self ):\n",
        "        # yield scrapy.Request(url = 'https://www.latimes.com/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/entertainment/music/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        music_headlines = []\n",
        "        lat_music_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        for music_headline in lat_music_headlines:\n",
        "            music_headlines.append(music_headline)\n",
        "        print(music_headlines)        \n",
        "\n",
        "timeout = 120\n",
        "\n",
        "def run_spider():\n",
        "    l.stop()\n",
        "    runner = CrawlerRunner(get_project_settings())\n",
        "    d = runner.crawl(LAtimesSpider)\n",
        "    d = runner.join()\n",
        "    d.addBoth(lambda _: l.start(timeout, False))\n",
        "\n",
        "l = task.LoopingCall(run_spider)\n",
        "l.start(timeout)\n",
        "\n",
        "reactor.run()\n",
        "\n",
        "\n",
        "\n",
        "# use next line instead of 'd = runner.crawl(LAtimesSpider)' if running more than one spider\n",
        "# d = runner.join()\n",
        "\n",
        "# put everything in a while True loop to run every 10 min (600 sec)\n",
        "# time.sleep(600)\n",
        "\n",
        "# can use schedule module to set when it runs\n",
        "\n",
        "\n",
        "\n",
        "# process = CrawlerProcess()\n",
        "# process.crawl(LAtimesSpider)\n",
        "# process.start()\n",
        "#\n",
        "# scrapy crawl latspider -o latspider.json\n",
        "\n",
        "from collections import Counter\n",
        "music = ['Anderson .Paak loves L.A. What does he do when it stops loving him back?', 'At Chella fest, Indio locals enjoy Coachella’s Latin lineup closer to home', 'Rapper Kodak Black arrested at U.S. border after guns and weed found, police say', \"Beyoncé has a surprise: 'Homecoming: The Live Album'\", 'Coachella showcased the power and joy of dance', 'Beyoncé’s ‘Homecoming’ Netflix doc captures an icon at her radical peak', 'The Game, Snoop Dogg push for Laura Ingraham’s firing after Nipsey Hussle diss', 'Nipsey Hussle memorial begins with beauty but has an ugly epilogue']\n",
        "musicstr = ''.join(music)\n",
        "split_it = musicstr.split()\n",
        "Counter = Counter(split_it)\n",
        "top_ten = Counter.most_common(10)\n",
        "print(top_ten)\n",
        "\n",
        "# [('at', 2), ('Hussle', 2), ('has', 2), ('after', 2), ('and', 2), ('an', 2), ('Chella', 1), ('of', 1), ('Anderson', 1), ('does', 1)]\n",
        "\n",
        "import nltk\n",
        "tokens = nltk.word_tokenize(musicstr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import asyncio\n",
        "import websockets\n",
        "import json\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# crawl latimes\n",
        "class LAtimesSpider(scrapy.Spider):\n",
        "    name = \"latspider\"\n",
        "    def start_requests(self):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/', callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        headlines = []\n",
        "        #lat_title = response.css('h5 > a::attr(href)::text').extract()\n",
        "        lat_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "        # for headline in lat_headlines:\n",
        "            # headlines.append(headline)\n",
        "\n",
        "        # LAtimesSpider.parse_headlines.headlinesOut = f\"{headlines}!\"\n",
        "\n",
        "# allowed_domains = ['www.latimes.com/']\n",
        "# start_urls = ['https://www.latimes.com/', 'https://www.latimes.com/entertainment/music/']\n",
        "\n",
        "# lat_headlines = response.css('h5 > a::attr(href)::text').extract()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dCVo7oT3-Qz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "78a47e69-287b-41aa-8864-43424e8f8267"
      },
      "source": [
        "\n",
        "\n",
        "from twisted.internet import reactor\n",
        "from twisted.internet import task\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "class LAtimesmusicSpider(scrapy.Spider):\n",
        "    name = \"latmusicspider\"\n",
        "    download_delay = 5.0\n",
        "    def start_requests( self ):\n",
        "        yield scrapy.Request(url = 'https://www.latimes.com/entertainment/music/', meta = {'dont_merge_cookies': True}, callback = self.parse_headlines)\n",
        "    def parse_headlines(self, response):\n",
        "        links = response.xpath('//div/div/h5/a/@href').extract()\n",
        "        for link in links:\n",
        "            yield response.follow( url = link, callback = self.parse_article )        \n",
        "        # music_headlines = []\n",
        "        # lat_music_headlines = response.xpath('//div/h5/a/text()').extract()\n",
        "    def parse_article(self, response):\n",
        "          # headline = response.xpath('//h1/text()').extract() #put here 4/9/20 KSS\n",
        "          # article = response.xpath('//p/text()').extract()\n",
        "          # image = response.xpath('//div/figure/div/img/@src').extract()\n",
        "        yield {\n",
        "          headline = response.xpath('//h1/text()').extract()\n",
        "          article = response.xpath('//p/text()').extract()\n",
        "          image = response.xpath('//div/figure/div/img/@src').extract()\n",
        "          }\n",
        "        for headline, image, article in zip(headline, image, article):\n",
        "            lat_dict[headline] = image, article       \n",
        "        for music_headline in lat_music_headlines:\n",
        "            music_headlines.append(music_headline)\n",
        "        print(music_headlines)\n",
        "\n",
        "lat_dict = dict()\n",
        "\n",
        "timeout = 120\n",
        "\n",
        "def run_spider():\n",
        "    l.stop()\n",
        "    runner = CrawlerRunner(get_project_settings())\n",
        "    d = runner.crawl(LAtimesSpider)\n",
        "    d = runner.join()\n",
        "    d.addBoth(lambda _: l.start(timeout, False))\n",
        "\n",
        "l = task.LoopingCall(run_spider)\n",
        "l.start(timeout)\n",
        "\n",
        "reactor.run()\n",
        "\n",
        "print(lat_dict)\n",
        "\n",
        "# def parse( self, response ):\n",
        "#     links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
        "#     # Follow each of the extracted links\n",
        "#     for link in links:\n",
        "#       yield response.follow( url = link, callback = self.parse_descr )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-072430b1a4f2>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    headline = response.xpath('//h1/text()').extract()\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}